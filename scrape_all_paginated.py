"""
NUFORC UFO å®Œæ•´æ•°æ®çˆ¬è™«ï¼ˆä½¿ç”¨Seleniumå¤„ç†åˆ†é¡µï¼‰
ä» subndx/?id=all é¡µé¢è·å–æ‰€æœ‰1586é¡µï¼Œçº¦158574æ¡è®°å½•
"""
import time
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from urllib.parse import urljoin
import re
from tqdm import tqdm

# æ·»åŠ webdriver-manageræ”¯æŒ
try:
    from webdriver_manager.chrome import ChromeDriverManager
    WEBDRIVER_MANAGER_AVAILABLE = True
except ImportError:
    WEBDRIVER_MANAGER_AVAILABLE = False
    print("âš ï¸ webdriver-manageræœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip3 install webdriver-manager")


class UFOPaginatedScraper:
    def __init__(self, headless=True):
        self.base_url = "https://nuforc.org"
        self.all_page_url = "https://nuforc.org/subndx/?id=all"
        self.headless = headless
        self.driver = None
        self.all_data = []
        
    def setup_driver(self):
        """è®¾ç½®Selenium WebDriverï¼ˆä½¿ç”¨webdriver-managerè‡ªåŠ¨ç®¡ç†ï¼‰"""
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        try:
            if WEBDRIVER_MANAGER_AVAILABLE:
                # ä½¿ç”¨webdriver-managerè‡ªåŠ¨ä¸‹è½½å’Œç®¡ç†ChromeDriver
                print("æ­£åœ¨ä½¿ç”¨webdriver-managerè‡ªåŠ¨ä¸‹è½½ChromeDriver...")
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                print("âœ… WebDriveråˆå§‹åŒ–æˆåŠŸ")
            else:
                # å›é€€åˆ°ç³»ç»ŸPATHä¸­çš„chromedriver
                self.driver = webdriver.Chrome(options=chrome_options)
            return True
        except Exception as e:
            print(f"âŒ Chrome driveråˆå§‹åŒ–å¤±è´¥: {e}")
            if not WEBDRIVER_MANAGER_AVAILABLE:
                print("\nğŸ’¡ æç¤ºï¼šè¯·è¿è¡Œ: pip3 install webdriver-manager")
            return False
    
    def is_yellow_background(self, element):
        """æ£€æµ‹å…ƒç´ æ˜¯å¦æœ‰é»„è‰²æˆ–æ·¡é»„è‰²èƒŒæ™¯"""
        if not element:
            return False
        
        style = element.get('style', '')
        bgcolor = element.get('bgcolor', '')
        all_color_info = (style + ' ' + bgcolor).lower()
        
        yellow_keywords = [
            'yellow', '#ffff00', '#ffffc0', '#ffffcc', '#ffff99',
            '#ffffe0', '#ffffd0', '#ffffb0', '#fffacd', '#fff8dc',
            '#ffeb3b', '#ffc107', 'rgb(255, 255, 0)', 'rgb(255, 255, 192)',
            'rgb(255, 255, 204)', 'rgb(255, 255, 176)', 'rgb(255, 255, 224)',
        ]
        
        for keyword in yellow_keywords:
            if keyword in all_color_info:
                return True
        
        rgb_match = re.search(r'rgb\((\d+),\s*(\d+),\s*(\d+)\)', all_color_info)
        if rgb_match:
            r, g, b = int(rgb_match.group(1)), int(rgb_match.group(2)), int(rgb_match.group(3))
            if r > 200 and g > 200 and b < 200:
                return True
        
        return False
    
    def parse_table_page(self):
        """è§£æå½“å‰é¡µé¢çš„è¡¨æ ¼æ•°æ®"""
        try:
            # ç­‰å¾…è¡¨æ ¼åŠ è½½
            wait = WebDriverWait(self.driver, 30)
            wait.until(EC.presence_of_element_located((By.TAG_NAME, "table")))
            
            # è·å–é¡µé¢HTML
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            table = soup.find('table')
            
            if not table:
                return []
            
            rows = table.find_all('tr')
            if len(rows) < 2:
                return []
            
            # è§£æè¡¨å¤´
            header_row = rows[0]
            header_cells = header_row.find_all(['th', 'td'])
            column_indices = {}
            
            for idx, cell in enumerate(header_cells):
                text = cell.get_text(strip=True).lower()
                if 'occurred' in text or 'date' in text:
                    column_indices['date'] = idx
                elif 'city' in text:
                    column_indices['city'] = idx
                elif 'state' in text:
                    column_indices['state'] = idx
                elif 'shape' in text:
                    column_indices['shape'] = idx
                elif 'summary' in text:
                    column_indices['summary'] = idx
                elif 'media' in text:
                    column_indices['media'] = idx
                elif 'link' in text or 'report' in text:
                    column_indices['link'] = idx
            
            # è§£ææ•°æ®è¡Œ
            page_data = []
            for row in rows[1:]:
                cells = row.find_all('td')
                if len(cells) < len(header_cells):
                    continue
                
                # æå–å„å­—æ®µ
                date = cells[column_indices.get('date', 0)].get_text(strip=True) if 'date' in column_indices else ''
                city = cells[column_indices.get('city', 1)].get_text(strip=True) if 'city' in column_indices else ''
                state = cells[column_indices.get('state', 2)].get_text(strip=True) if 'state' in column_indices else ''
                shape = cells[column_indices.get('shape', 5)].get_text(strip=True) if 'shape' in column_indices else ''
                summary = cells[column_indices.get('summary', 6)].get_text(strip=True) if 'summary' in column_indices else ''
                media = cells[column_indices.get('media', 8)].get_text(strip=True) if 'media' in column_indices else ''
                
                # æå–Reporté“¾æ¥å¹¶æ£€æµ‹Tier
                # æ–¹æ³•ï¼šéå†æ•´è¡Œçš„æ‰€æœ‰å•å…ƒæ ¼æŸ¥æ‰¾é“¾æ¥ï¼ˆä¸ä¾èµ–ç‰¹å®šåˆ—ï¼‰
                report_link = ''
                is_high_tier = False
                link_cell = None
                
                # éå†æ‰€æœ‰å•å…ƒæ ¼æŸ¥æ‰¾åŒ…å«é“¾æ¥çš„å•å…ƒæ ¼
                for cell in cells:
                    link_tag = cell.find('a', href=True)
                    if link_tag:
                        href = link_tag.get('href', '')
                        if '/sighting/?id=' in href:
                            report_link = urljoin(self.base_url, href)
                            link_cell = cell
                            break
                
                # å¦‚æœæ‰¾åˆ°äº†é“¾æ¥å•å…ƒæ ¼ï¼Œæ£€æµ‹Tier
                if link_cell:
                    # æ£€æŸ¥å•å…ƒæ ¼èƒŒæ™¯è‰²
                    if self.is_yellow_background(link_cell):
                        is_high_tier = True
                    
                    # æ£€æŸ¥é“¾æ¥å…ƒç´ 
                    link_tag = link_cell.find('a', href=True)
                    if link_tag:
                        link_text = link_tag.get_text(strip=True)
                        # æ£€æŸ¥é“¾æ¥æ–‡æœ¬ä¸­çš„ç¬¦å·ï¼ˆTieræ ‡è®°ï¼‰
                        if '!' in link_text:
                            is_high_tier = True
                        elif link_text.endswith('.') or link_text == 'Open .' or 'Open .' in link_text:
                            is_high_tier = True
                        # æ£€æŸ¥é“¾æ¥å…ƒç´ æœ¬èº«çš„èƒŒæ™¯è‰²
                        if self.is_yellow_background(link_tag):
                            is_high_tier = True
                
                page_data.append({
                    'Date': date,
                    'City': city,
                    'State': state,
                    'Shape': shape,
                    'Summary': summary,
                    'Media': media,
                    'Report_Link': report_link,
                    'Is_High_Tier': is_high_tier
                })
            
            return page_data
            
        except Exception as e:
            print(f"è§£æé¡µé¢å¤±è´¥: {e}")
            return []
    
    def get_total_pages(self):
        """è·å–æ€»é¡µæ•°"""
        try:
            # ç­‰å¾…åˆ†é¡µä¿¡æ¯åŠ è½½
            wait = WebDriverWait(self.driver, 30)
            # æŸ¥æ‰¾åˆ†é¡µä¿¡æ¯ï¼Œé€šå¸¸åœ¨"Showing X to Y of Z entries"è¿™æ ·çš„æ–‡æœ¬ä¸­
            # æˆ–è€…æŸ¥æ‰¾æœ€åä¸€é¡µçš„é¡µç 
            time.sleep(5)  # ç­‰å¾…DataTableså®Œå…¨åŠ è½½
            
            # å°è¯•é€šè¿‡JavaScriptè·å–æ€»é¡µæ•°
            try:
                # DataTablesé€šå¸¸ä¼šåœ¨windowä¸Šæš´éœ²è¡¨æ ¼å¯¹è±¡
                total_pages = self.driver.execute_script("""
                    if (typeof jQuery !== 'undefined' && jQuery.fn.dataTable) {
                        var table = jQuery('#table_1').DataTable();
                        if (table) {
                            return table.page.info().pages;
                        }
                    }
                    return null;
                """)
                if total_pages:
                    return int(total_pages)
            except:
                pass
            
            # å¤‡ç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾åˆ†é¡µæ§ä»¶ä¸­çš„æœ€åä¸€é¡µ
            try:
                pagination_elements = self.driver.find_elements(By.CSS_SELECTOR, ".dataTables_paginate a")
                page_numbers = []
                for elem in pagination_elements:
                    text = elem.text.strip()
                    if text.isdigit():
                        page_numbers.append(int(text))
                if page_numbers:
                    return max(page_numbers)
            except:
                pass
            
            # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼1586
            print("âš ï¸ æ— æ³•è‡ªåŠ¨æ£€æµ‹æ€»é¡µæ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼1586")
            return 1586
            
        except Exception as e:
            print(f"è·å–æ€»é¡µæ•°å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼1586")
            return 1586
    
    def go_to_page(self, page_num):
        """è·³è½¬åˆ°æŒ‡å®šé¡µé¢"""
        try:
            wait = WebDriverWait(self.driver, 30)
            
            # æ–¹æ³•1: ä½¿ç”¨DataTables APIè·³è½¬
            try:
                self.driver.execute_script(f"""
                    if (typeof jQuery !== 'undefined' && jQuery.fn.dataTable) {{
                        var table = jQuery('#table_1').DataTable();
                        if (table) {{
                            table.page({page_num - 1}).draw('page');
                            return true;
                        }}
                    }}
                    return false;
                """)
                time.sleep(2)  # ç­‰å¾…é¡µé¢åŠ è½½
                return True
            except:
                pass
            
            # æ–¹æ³•2: ç‚¹å‡»åˆ†é¡µæŒ‰é’®
            try:
                # æŸ¥æ‰¾åŒ…å«ç›®æ ‡é¡µç çš„é“¾æ¥
                page_link = self.driver.find_element(By.XPATH, f"//a[contains(@class, 'paginate_button') and text()='{page_num}']")
                self.driver.execute_script("arguments[0].click();", page_link)
                time.sleep(2)
                return True
            except:
                pass
            
            # æ–¹æ³•3: ä½¿ç”¨"ä¸‹ä¸€é¡µ"æŒ‰é’®é€æ­¥ç¿»é¡µ
            # è¿™é‡Œéœ€è¦çŸ¥é“å½“å‰é¡µç ï¼Œç„¶åç‚¹å‡»å¤šæ¬¡"ä¸‹ä¸€é¡µ"
            return False
            
        except Exception as e:
            print(f"è·³è½¬åˆ°ç¬¬{page_num}é¡µå¤±è´¥: {e}")
            return False
    
    def scrape_all(self):
        """ä¸»çˆ¬å–å‡½æ•°"""
        print("=" * 60)
        print("NUFORC UFO å®Œæ•´æ•°æ®çˆ¬è™«å¯åŠ¨ï¼ˆä½¿ç”¨Seleniumï¼‰")
        print("ç›®æ ‡ï¼šè·å–æ‰€æœ‰1586é¡µï¼Œçº¦158574æ¡è®°å½•")
        print("=" * 60)
        
        # 1. åˆå§‹åŒ–æµè§ˆå™¨
        if not self.setup_driver():
            return
        
        try:
            # 2. è®¿é—®ç›®æ ‡é¡µé¢
            print(f"\næ­£åœ¨è®¿é—®: {self.all_page_url}")
            self.driver.get(self.all_page_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            print("ç­‰å¾…é¡µé¢åŠ è½½...")
            time.sleep(10)  # ç»™DataTablesè¶³å¤Ÿæ—¶é—´åŠ è½½
            
            # 3. è·å–æ€»é¡µæ•°
            total_pages = self.get_total_pages()
            print(f"\nâœ… æ£€æµ‹åˆ°æ€»é¡µæ•°: {total_pages}")
            
            # 4. éå†æ‰€æœ‰é¡µé¢
            print(f"\nå¼€å§‹æŠ“å–æ•°æ®...")
            for page_num in tqdm(range(1, total_pages + 1), desc="æŠ“å–é¡µé¢", unit="é¡µ"):
                # è·³è½¬åˆ°ç›®æ ‡é¡µé¢
                if page_num > 1:
                    if not self.go_to_page(page_num):
                        print(f"âš ï¸ æ— æ³•è·³è½¬åˆ°ç¬¬{page_num}é¡µï¼Œè·³è¿‡")
                        continue
                
                # è§£æå½“å‰é¡µé¢
                page_data = self.parse_table_page()
                self.all_data.extend(page_data)
                
                # æ¯10é¡µä¿å­˜ä¸€æ¬¡ï¼ˆé˜²æ­¢æ•°æ®ä¸¢å¤±ï¼‰
                if page_num % 10 == 0:
                    self.save_partial_data()
                    print(f"\n[è¿›åº¦] å·²å¤„ç† {page_num}/{total_pages} é¡µï¼Œå·²è·å– {len(self.all_data)} æ¡è®°å½•")
            
            # 5. ä¿å­˜æœ€ç»ˆæ•°æ®
            self.save_final_data()
            
            print("\n" + "=" * 60)
            print("âœ… æŠ“å–å®Œæˆï¼")
            print(f"ğŸ“Š æ€»å…±è·å–äº† {len(self.all_data)} æ¡è®°å½•")
            print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜è‡³ ufo_data_tiered_full.csv")
            print("=" * 60)
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å·²æŠ“å–çš„æ•°æ®...")
            self.save_final_data()
        except Exception as e:
            print(f"\nâŒ æŠ“å–è¿‡ç¨‹å‡ºé”™: {e}")
            self.save_final_data()
        finally:
            if self.driver:
                self.driver.quit()
    
    def save_partial_data(self):
        """ä¿å­˜éƒ¨åˆ†æ•°æ®ï¼ˆä¸­é—´ä¿å­˜ï¼‰"""
        if not self.all_data:
            return
        df = pd.DataFrame(self.all_data)
        df.to_csv('ufo_data_tiered_partial.csv', index=False, encoding='utf-8')
    
    def save_final_data(self):
        """ä¿å­˜æœ€ç»ˆæ•°æ®"""
        if not self.all_data:
            print("æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            return
        
        df = pd.DataFrame(self.all_data)
        
        # å»é‡ï¼šå¦‚æœReport_Linkæœ‰ç©ºå€¼ï¼Œä½¿ç”¨Date+City+Stateç»„åˆå»é‡
        if df['Report_Link'].isna().all():
            # æ‰€æœ‰Report_Linkéƒ½æ˜¯ç©ºçš„ï¼Œä½¿ç”¨å…¶ä»–å­—æ®µç»„åˆå»é‡
            df = df.drop_duplicates(subset=['Date', 'City', 'State', 'Shape'], keep='first')
        else:
            # ä½¿ç”¨Report_Linkå»é‡
            df = df.drop_duplicates(subset=['Report_Link'], keep='first')
        
        # ä¿å­˜
        output_file = 'ufo_data_tiered_full.csv'
        df.to_csv(output_file, index=False, encoding='utf-8')
        
        print(f"\nâœ… æ•°æ®å·²ä¿å­˜è‡³ {output_file}")
        print(f"ğŸ“Š æ€»è®°å½•æ•°: {len(df)}")
        print(f"â­ Tier 1/2: {df['Is_High_Tier'].sum()} æ¡")


def main():
    scraper = UFOPaginatedScraper(headless=False)  # è®¾ç½®ä¸ºFalseå¯ä»¥çœ‹åˆ°æµè§ˆå™¨æ“ä½œè¿‡ç¨‹
    scraper.scrape_all()


if __name__ == "__main__":
    main()
