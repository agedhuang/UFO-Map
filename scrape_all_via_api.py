"""
NUFORC UFO å®Œæ•´æ•°æ®çˆ¬è™«ï¼ˆé€šè¿‡DataTables APIè·å–æ‰€æœ‰1586é¡µï¼‰
ç›´æ¥è°ƒç”¨DataTablesçš„æœåŠ¡å™¨ç«¯APIï¼Œè·å–æ‰€æœ‰çº¦158574æ¡è®°å½•
"""
import requests
import pandas as pd
import time
import json
import urllib3
from tqdm import tqdm
from urllib.parse import urljoin
import re

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class UFOAPIScraper:
    def __init__(self):
        self.base_url = "https://nuforc.org"
        self.all_page_url = "https://nuforc.org/subndx/?id=all"
        self.api_url = "https://nuforc.org/wp-admin/admin-ajax.php"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Accept-Language': 'en-US,en;q=0.9',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'X-Requested-With': 'XMLHttpRequest',
            'Origin': 'https://nuforc.org',
            'Referer': 'https://nuforc.org/subndx/?id=all'
        })
        self.all_data = []
        
    def get_session_cookies(self):
        """å…ˆè®¿é—®ä¸»é¡µè·å–å¿…è¦çš„cookieså’Œsessionä¿¡æ¯"""
        try:
            print("æ­£åœ¨è·å–sessionä¿¡æ¯...")
            response = self.session.get(self.all_page_url, timeout=30, verify=False)
            response.raise_for_status()
            
            # ä»HTMLä¸­æå–å¯èƒ½çš„tokenæˆ–nonce
            if 'wpDataTables' in response.text:
                print("âœ… Sessionåˆå§‹åŒ–æˆåŠŸ")
                return True
            return False
        except Exception as e:
            print(f"âš ï¸ Sessionåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def is_yellow_background(self, element_style):
        """æ£€æµ‹æ ·å¼å­—ç¬¦ä¸²æ˜¯å¦åŒ…å«é»„è‰²èƒŒæ™¯"""
        if not element_style:
            return False
        
        style = element_style.lower()
        yellow_keywords = [
            'yellow', '#ffff00', '#ffffc0', '#ffffcc', '#ffff99',
            '#ffffe0', '#ffffd0', '#ffffb0', '#fffacd', '#fff8dc',
            '#ffeb3b', '#ffc107', 'rgb(255, 255, 0)', 'rgb(255, 255, 192)',
            'rgb(255, 255, 204)', 'rgb(255, 255, 176)', 'rgb(255, 255, 224)',
        ]
        
        for keyword in yellow_keywords:
            if keyword in style:
                return True
        
        rgb_match = re.search(r'rgb\((\d+),\s*(\d+),\s*(\d+)\)', style)
        if rgb_match:
            r, g, b = int(rgb_match.group(1)), int(rgb_match.group(2)), int(rgb_match.group(3))
            if r > 200 and g > 200 and b < 200:
                return True
        
        return False
    
    def fetch_page(self, start=0, length=100):
        """ä»APIè·å–ä¸€é¡µæ•°æ®"""
        try:
            params = {
                'action': 'get_wdtable',
                'table_id': '1',
                'wdt_var1': 'Post',
                'wdt_var2': '-1'
            }
            
            data = {
                'draw': '1',
                'start': str(start),
                'length': str(length),
                'order[0][column]': '1',
                'order[0][dir]': 'desc',
                'search[value]': '',
                'search[regex]': 'false'
            }
            
            response = self.session.post(
                self.api_url,
                params=params,
                data=data,
                timeout=30,
                verify=False
            )
            
            if response.status_code == 200:
                try:
                    result = response.json()
                    return result
                except json.JSONDecodeError:
                    # å¦‚æœè¿”å›çš„æ˜¯HTMLé”™è¯¯é¡µé¢
                    if len(response.text) < 500:
                        print(f"âš ï¸ APIè¿”å›éJSONå“åº”: {response.text[:200]}")
                    return None
            else:
                print(f"âš ï¸ APIè¿”å›çŠ¶æ€ç : {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âš ï¸ è·å–é¡µé¢æ•°æ®å¤±è´¥ (start={start}): {e}")
            return None
    
    def parse_api_data(self, api_result):
        """è§£æAPIè¿”å›çš„æ•°æ®"""
        if not api_result or 'data' not in api_result:
            return []
        
        rows_data = api_result['data']
        parsed_data = []
        
        for row in rows_data:
            # DataTables APIè¿”å›çš„æ•°æ®é€šå¸¸æ˜¯æ•°ç»„ï¼Œéœ€è¦æ ¹æ®åˆ—ç´¢å¼•è§£æ
            # åˆ—é¡ºåºé€šå¸¸æ˜¯: Date, City, State, Shape, Summary, Media, Link, ...
            if len(row) < 8:
                continue
            
            date = str(row[0]).strip() if row[0] else ''
            city = str(row[1]).strip() if row[1] else ''
            state = str(row[2]).strip() if row[2] else ''
            shape = str(row[5]).strip() if row[5] else ''
            summary = str(row[6]).strip() if row[6] else ''
            media = str(row[7]).strip() if row[7] else ''
            
            # æå–é“¾æ¥ï¼ˆé€šå¸¸åœ¨æŸä¸ªåˆ—ä¸­ï¼Œå¯èƒ½æ˜¯HTMLæ ¼å¼ï¼‰
            report_link = ''
            is_high_tier = False
            
            # æŸ¥æ‰¾é“¾æ¥åˆ—ï¼ˆé€šå¸¸åŒ…å«<a href="/sighting/?id=...">ï¼‰
            for cell in row:
                if isinstance(cell, str) and '/sighting/?id=' in cell:
                    # æå–URL
                    link_match = re.search(r'href=["\']([^"\']*\/sighting\/\?id=\d+)["\']', cell)
                    if link_match:
                        report_link = urljoin(self.base_url, link_match.group(1))
                    
                    # æ£€æŸ¥Tieræ ‡è®°
                    if '!' in cell:
                        is_high_tier = True
                    elif 'Open .' in cell or (cell.endswith('.') and 'Open' in cell):
                        is_high_tier = True
                    
                    # æ£€æŸ¥é»„è‰²èƒŒæ™¯
                    if self.is_yellow_background(cell):
                        is_high_tier = True
                    break
            
            parsed_data.append({
                'Date': date,
                'City': city,
                'State': state,
                'Shape': shape,
                'Summary': summary,
                'Media': media,
                'Report_Link': report_link,
                'Is_High_Tier': is_high_tier
            })
        
        return parsed_data
    
    def scrape_all(self):
        """ä¸»çˆ¬å–å‡½æ•°"""
        print("=" * 60)
        print("NUFORC UFO å®Œæ•´æ•°æ®çˆ¬è™«å¯åŠ¨ï¼ˆé€šè¿‡DataTables APIï¼‰")
        print("ç›®æ ‡ï¼šè·å–æ‰€æœ‰1586é¡µï¼Œçº¦158574æ¡è®°å½•")
        print("=" * 60)
        
        # 1. åˆå§‹åŒ–session
        if not self.get_session_cookies():
            print("âŒ Sessionåˆå§‹åŒ–å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return
        
        # 2. å…ˆè·å–ç¬¬ä¸€é¡µä»¥ç¡®å®šæ€»è®°å½•æ•°
        print("\næ­£åœ¨è·å–ç¬¬ä¸€é¡µæ•°æ®ä»¥ç¡®å®šæ€»è®°å½•æ•°...")
        first_page = self.fetch_page(0, 100)
        
        if not first_page:
            print("âŒ æ— æ³•è·å–æ•°æ®ï¼ŒAPIè°ƒç”¨å¤±è´¥")
            print("ğŸ’¡ æç¤ºï¼šå¯èƒ½éœ€è¦ä½¿ç”¨Seleniumæ–¹å¼ï¼ˆè¿è¡Œ scrape_all_paginated.pyï¼‰")
            return
        
        total_records = first_page.get('recordsTotal', 158574)
        records_per_page = first_page.get('length', 100) if 'length' in first_page else 100
        total_pages = (total_records + records_per_page - 1) // records_per_page
        
        print(f"âœ… æ€»è®°å½•æ•°: {total_records}")
        print(f"âœ… æ¯é¡µè®°å½•æ•°: {records_per_page}")
        print(f"âœ… æ€»é¡µæ•°: {total_pages}")
        
        # è§£æç¬¬ä¸€é¡µæ•°æ®
        first_page_data = self.parse_api_data(first_page)
        self.all_data.extend(first_page_data)
        print(f"âœ… ç¬¬ä¸€é¡µè§£æå®Œæˆï¼Œè·å¾— {len(first_page_data)} æ¡è®°å½•\n")
        
        # 3. éå†å‰©ä½™é¡µé¢
        print("å¼€å§‹æŠ“å–å‰©ä½™é¡µé¢...")
        for page_num in tqdm(range(1, total_pages), desc="æŠ“å–é¡µé¢", unit="é¡µ"):
            start = page_num * records_per_page
            page_result = self.fetch_page(start, records_per_page)
            
            if page_result:
                page_data = self.parse_api_data(page_result)
                self.all_data.extend(page_data)
            else:
                print(f"\nâš ï¸ ç¬¬{page_num + 1}é¡µè·å–å¤±è´¥ï¼Œè·³è¿‡")
            
            # æ¯10é¡µä¼‘æ¯ä¸€ä¸‹
            if (page_num + 1) % 10 == 0:
                time.sleep(0.5)
                # æ¯100é¡µä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
                if (page_num + 1) % 100 == 0:
                    self.save_partial_data()
                    print(f"\n[è¿›åº¦] å·²å¤„ç† {page_num + 1}/{total_pages} é¡µï¼Œå·²è·å– {len(self.all_data)} æ¡è®°å½•")
        
        # 4. ä¿å­˜æœ€ç»ˆæ•°æ®
        self.save_final_data()
        
        print("\n" + "=" * 60)
        print("âœ… æŠ“å–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»å…±è·å–äº† {len(self.all_data)} æ¡è®°å½•")
        print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜è‡³ ufo_data_tiered_full.csv")
        print("=" * 60)
    
    def save_partial_data(self):
        """ä¿å­˜éƒ¨åˆ†æ•°æ®ï¼ˆä¸­é—´ä¿å­˜ï¼‰"""
        if not self.all_data:
            return
        df = pd.DataFrame(self.all_data)
        df = df.drop_duplicates(subset=['Report_Link'], keep='first')
        df.to_csv('ufo_data_tiered_partial.csv', index=False, encoding='utf-8')
    
    def save_final_data(self):
        """ä¿å­˜æœ€ç»ˆæ•°æ®"""
        if not self.all_data:
            print("æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            return
        
        df = pd.DataFrame(self.all_data)
        
        # å»é‡
        df = df.drop_duplicates(subset=['Report_Link'], keep='first')
        
        # ä¿å­˜
        output_file = 'ufo_data_tiered_full.csv'
        df.to_csv(output_file, index=False, encoding='utf-8')
        
        print(f"\nâœ… æ•°æ®å·²ä¿å­˜è‡³ {output_file}")
        print(f"ğŸ“Š æ€»è®°å½•æ•°: {len(df)}")
        print(f"â­ Tier 1/2: {df['Is_High_Tier'].sum()} æ¡")
        print(f"ğŸ“¸ Media=Y: {len(df[df['Media'] == 'Y'])} æ¡")


def main():
    scraper = UFOAPIScraper()
    scraper.scrape_all()


if __name__ == "__main__":
    main()
