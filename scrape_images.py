"""
NUFORC UFO å›¾ç‰‡çˆ¬è™«
ä»è¯¦æƒ…é¡µæŠ“å–æ‰€æœ‰å¸¦å›¾ç‰‡çš„æŠ¥å‘Šï¼ŒåŒ…æ‹¬å›¾ç‰‡URLã€æŠ¥å‘Šä¿¡æ¯å’ŒTierç­‰çº§
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from urllib.parse import urljoin
from tqdm import tqdm


class UFOImageScraper:
    def __init__(self):
        self.base_url = "https://nuforc.org"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.all_images = []
        
    def read_tiered_data(self):
        """
        è¯»å–å·²æœ‰çš„Tieræ•°æ®æ–‡ä»¶
        """
        try:
            df = pd.read_csv('ufo_data_tiered.csv')
            print(f"è¯»å–åˆ° {len(df)} æ¡Tieræ•°æ®")
            return df
        except Exception as e:
            print(f"è¯»å–Tieræ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def extract_image_from_detail_page(self, report_url):
        """
        ä»è¯¦æƒ…é¡µæå–å›¾ç‰‡URLå’Œç›¸å…³ä¿¡æ¯
        """
        try:
            response = self.session.get(report_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–åŸºæœ¬ä¿¡æ¯
            date = ''
            city = ''
            state = ''
            shape = ''
            summary = ''
            description = ''
            
            # æŸ¥æ‰¾æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼Œå°è¯•æå–å­—æ®µ
            text_content = soup.get_text()
            
            # æå–å›¾ç‰‡
            images = []
            img_tags = soup.find_all('img')
            
            for img in img_tags:
                src = img.get('src', '')
                if src:
                    # è·³è¿‡logoå’Œå›¾æ ‡
                    if 'logo' in src.lower() or 'icon' in src.lower() or 'button' in src.lower():
                        continue
                    
                    # æ„å»ºå®Œæ•´URL
                    if not src.startswith('http'):
                        full_url = urljoin(self.base_url, src)
                    else:
                        full_url = src
                    
                    images.append(full_url)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œè¿”å›None
            if not images:
                return None
            
            # å°è¯•ä»é¡µé¢æå–æŠ¥å‘Šä¿¡æ¯
            # æŸ¥æ‰¾åŒ…å«æ—¥æœŸçš„æ–‡æœ¬
            date_match = re.search(r'(\d{1,2}/\d{1,2}/\d{4})', text_content)
            if date_match:
                date = date_match.group(1)
            
            # æŸ¥æ‰¾åŸå¸‚å’Œå·
            city_match = re.search(r'City[:\s]+([^,\n]+)', text_content, re.IGNORECASE)
            if city_match:
                city = city_match.group(1).strip()
            
            state_match = re.search(r'State[:\s]+([A-Z]{2})', text_content, re.IGNORECASE)
            if state_match:
                state = state_match.group(1)
            
            # æŸ¥æ‰¾Shape
            shape_match = re.search(r'Shape[:\s]+([^\n]+)', text_content, re.IGNORECASE)
            if shape_match:
                shape = shape_match.group(1).strip()[:50]
            
            # æå–æè¿°ï¼ˆå‰500å­—ç¬¦ï¼‰
            paragraphs = soup.find_all('p')
            description_texts = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
            description = ' '.join(description_texts)[:500]
            
            return {
                'images': images,
                'date': date,
                'city': city,
                'state': state,
                'shape': shape,
                'description': description,
                'report_url': report_url
            }
            
        except Exception as e:
            print(f"\næå–å›¾ç‰‡å¤±è´¥ ({report_url}): {e}")
            return None
    
    def scrape_all_images(self):
        """
        ä¸»çˆ¬å–å‡½æ•°ï¼šä»Tieræ•°æ®ä¸­æå–æ‰€æœ‰æŠ¥å‘Šé“¾æ¥ï¼Œç„¶åè®¿é—®è¯¦æƒ…é¡µæŠ“å–å›¾ç‰‡
        """
        print("=" * 60)
        print("NUFORC UFO å›¾ç‰‡çˆ¬è™«å¯åŠ¨")
        print("=" * 60)
        
        # 1. è¯»å–Tieræ•°æ®
        tier_df = self.read_tiered_data()
        if tier_df.empty:
            print("æœªæ‰¾åˆ°Tieræ•°æ®æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")
            return
        
        # 2. ç­›é€‰æœ‰Report_Linkçš„è®°å½•
        reports_with_links = tier_df[tier_df['Report_Link'].notna() & (tier_df['Report_Link'] != '')]
        print(f"\næ‰¾åˆ° {len(reports_with_links)} æ¡æœ‰é“¾æ¥çš„æŠ¥å‘Š")
        
        # 3. éå†æ‰€æœ‰æŠ¥å‘Šï¼Œæå–å›¾ç‰‡ï¼ˆ8åˆ†é’Ÿå†…å°½å¯èƒ½å¤šåœ°è·å–ï¼‰
        print("\nå¼€å§‹æŠ“å–å›¾ç‰‡...")
        print("âš ï¸ å¿«é€Ÿæ¨¡å¼ï¼š8åˆ†é’Ÿå†…å°½å¯èƒ½å¤šåœ°è·å–å›¾ç‰‡")
        import time as time_module
        start_time = time_module.time()
        time_limit = 8 * 60  # 8åˆ†é’Ÿ
        
        # ä¼˜å…ˆå¤„ç†Tier 1/2çš„æŠ¥å‘Š
        tier_reports = reports_with_links[reports_with_links['Is_High_Tier'] == True]
        normal_reports = reports_with_links[reports_with_links['Is_High_Tier'] != True]
        
        # å…ˆå¤„ç†TieræŠ¥å‘Š
        all_reports = pd.concat([tier_reports, normal_reports])
        
        for idx, row in tqdm(all_reports.iterrows(), total=len(all_reports), desc="æŠ“å–æŠ¥å‘Š", unit="ä¸ª"):
            # æ£€æŸ¥æ—¶é—´é™åˆ¶
            elapsed = time_module.time() - start_time
            if elapsed > time_limit:
                print(f"\nâ° æ—¶é—´é™åˆ¶åˆ°è¾¾ï¼ˆ8åˆ†é’Ÿï¼‰ï¼Œå·²è·å– {len(self.all_images)} å¼ å›¾ç‰‡")
                break
            
            report_url = row['Report_Link']
            is_high_tier = row.get('Is_High_Tier', False)
            
            # è®¿é—®è¯¦æƒ…é¡µæå–å›¾ç‰‡
            image_data = self.extract_image_from_detail_page(report_url)
            
            if image_data and image_data['images']:
                # ä¸ºæ¯å¼ å›¾ç‰‡åˆ›å»ºä¸€æ¡è®°å½•
                for img_url in image_data['images']:
                    self.all_images.append({
                        'Image_URL': img_url,
                        'Report_URL': report_url,
                        'Date': image_data.get('date', row.get('Date', '')),
                        'City': image_data.get('city', row.get('City', '')),
                        'State': image_data.get('state', row.get('State', '')),
                        'Shape': image_data.get('shape', row.get('Shape', '')),
                        'Summary': row.get('Summary', ''),
                        'Description': image_data.get('description', ''),
                        'Is_High_Tier': is_high_tier,
                        'Tier': 'Tier 1/2' if is_high_tier else 'Normal'
                    })
                    
                    # å¦‚æœå·²ç»è·å–äº†è¶³å¤Ÿçš„å›¾ç‰‡ï¼Œæå‰é€€å‡º
                    if len(self.all_images) >= 50:
                        break
            
            # æ¯æ¬¡è¯·æ±‚åä¼‘æ¯0.3ç§’ï¼ˆåŠ å¿«é€Ÿåº¦ï¼Œ8åˆ†é’Ÿå†…è·å–æ›´å¤šï¼‰
            time.sleep(0.3)
        
        # 4. ä¿å­˜æ•°æ®
        if not self.all_images:
            print("\næœªè·å–åˆ°ä»»ä½•å›¾ç‰‡æ•°æ®")
            return
        
        print("\næ­£åœ¨ä¿å­˜æ•°æ®...")
        df = pd.DataFrame(self.all_images)
        
        # ç¡®ä¿åˆ—çš„é¡ºåº
        columns_order = ['Image_URL', 'Report_URL', 'Date', 'City', 'State', 'Shape', 'Summary', 'Description', 'Is_High_Tier', 'Tier']
        df = df[columns_order]
        
        # ä¿å­˜æ•°æ®
        output_file = 'ufo_images.csv'
        df.to_csv(output_file, index=False, encoding='utf-8')
        
        # è¾“å‡ºç»Ÿè®¡
        print("\n" + "=" * 60)
        print("âœ… æŠ“å–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»å…±è·å–äº† {len(df)} å¼ å›¾ç‰‡")
        print(f"â­ Tier 1/2 å›¾ç‰‡: {df['Is_High_Tier'].sum()} å¼  ({df['Is_High_Tier'].sum()/len(df)*100:.2f}%)")
        print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜è‡³ {output_file}")
        print("=" * 60)


def main():
    scraper = UFOImageScraper()
    scraper.scrape_all_images()


if __name__ == "__main__":
    main()

