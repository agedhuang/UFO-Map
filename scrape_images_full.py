"""
NUFORC UFO å›¾ç‰‡å®Œæ•´çˆ¬è™«
ä» subndx/?id=all é¡µé¢åˆ†é¡µè·å–æ‰€æœ‰æŠ¥å‘Šæ•°æ®ï¼ˆ1586é¡µï¼Œçº¦158574æ¡ï¼‰
ç­›é€‰Media=Yçš„è®°å½•ï¼Œç„¶åè®¿é—®è¯¦æƒ…é¡µæŠ“å–å›¾ç‰‡å’Œæè¿°
æ”¯æŒæ–­ç‚¹ç»­ä¼ 
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import os
from urllib.parse import urljoin
from tqdm import tqdm


class UFOImageScraper:
    def __init__(self):
        self.base_url = "https://nuforc.org"
        self.all_page_url = "https://nuforc.org/subndx/?id=all"
        self.records_per_page = 100  # æ¯é¡µ100æ¡è®°å½•
        self.total_pages = 1586  # æ€»å…±1586é¡µ
        self.total_records = 158574  # æ€»å…±çº¦158574æ¡è®°å½•
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.all_images = []
    
    def is_yellow_background(self, element):
        """
        æ£€æµ‹å…ƒç´ æ˜¯å¦æœ‰é»„è‰²æˆ–æ·¡é»„è‰²èƒŒæ™¯
        æ£€æŸ¥styleå±æ€§å’Œbgcolorå±æ€§
        """
        if not element:
            return False
        
        # è·å–styleå±æ€§å’Œbgcolorå±æ€§
        style = element.get('style', '')
        bgcolor = element.get('bgcolor', '')
        
        # åˆå¹¶æ‰€æœ‰é¢œè‰²ä¿¡æ¯
        all_color_info = (style + ' ' + bgcolor).lower()
        
        # æ£€æŸ¥é»„è‰²å…³é”®è¯ï¼ˆåŒ…æ‹¬å„ç§å¯èƒ½çš„é»„è‰²å˜ä½“ï¼‰
        yellow_keywords = [
            'yellow',
            '#ffff00',  # çº¯é»„è‰²
            '#ffffc0',  # æ·¡é»„è‰²
            '#ffffcc',  # æ·¡é»„è‰²å˜ä½“
            '#ffff99',  # æ·¡é»„è‰²å˜ä½“
            '#ffffe0',  # æ·¡é»„è‰²å˜ä½“
            '#ffffd0',  # æ·¡é»„è‰²å˜ä½“
            '#ffffb0',  # æ·¡é»„è‰²å˜ä½“
            '#fffacd',  # æŸ æª¬é›ªçººè‰²
            '#fff8dc',  # ç‰ç±³è‰²
            '#ffeb3b',  # äº®é»„è‰²
            '#ffc107',  # ç¥ç€è‰²ï¼ˆåé»„ï¼‰
            'rgb(255, 255, 0)',  # RGBé»„è‰²
            'rgb(255, 255, 192)',  # RGBæ·¡é»„è‰²
            'rgb(255, 255, 204)',  # RGBæ·¡é»„è‰²å˜ä½“
            'rgb(255, 255, 176)',  # RGBæ·¡é»„è‰²å˜ä½“
            'rgb(255, 255, 224)',  # RGBæ·¡é»„è‰²å˜ä½“
        ]
        
        for keyword in yellow_keywords:
            if keyword in all_color_info:
                return True
        
        # æ£€æŸ¥RGBæ ¼å¼ (rgb(255, 255, x) å…¶ä¸­x < 200è¡¨ç¤ºé»„è‰²)
        rgb_match = re.search(r'rgb\((\d+),\s*(\d+),\s*(\d+)\)', all_color_info)
        if rgb_match:
            r, g, b = int(rgb_match.group(1)), int(rgb_match.group(2)), int(rgb_match.group(3))
            # é»„è‰²ç‰¹å¾ï¼šRå’ŒGéƒ½å¾ˆé«˜(>200)ï¼ŒBè¾ƒä½(<200)
            if r > 200 and g > 200 and b < 200:
                return True
        
        return False
    
    def scrape_paginated_table(self, start=0):
        """
        ä½¿ç”¨BeautifulSoupè§£æåˆ†é¡µè¡¨æ ¼ï¼Œæ£€æµ‹Tier
        start: èµ·å§‹è®°å½•ä½ç½®ï¼ˆ0è¡¨ç¤ºç¬¬ä¸€é¡µï¼Œ100è¡¨ç¤ºç¬¬äºŒé¡µï¼Œä¾æ­¤ç±»æ¨ï¼‰
        """
        try:
            # æ„å»ºåˆ†é¡µURL
            page_url = f"{self.all_page_url}&start={start}"
            
            # å°è¯•å…ˆä½¿ç”¨SSLéªŒè¯
            try:
                response = self.session.get(page_url, timeout=30, verify=True)
            except Exception as e:
                import urllib3
                urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
                response = self.session.get(page_url, timeout=30, verify=False)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            table = soup.find('table')
            
            if not table:
                return []
            
            rows = table.find_all('tr')
            if len(rows) < 2:  # è‡³å°‘éœ€è¦è¡¨å¤´+1è¡Œæ•°æ®
                return []
            
            # è§£æè¡¨å¤´ï¼Œæ‰¾åˆ°å„åˆ—çš„ç´¢å¼•
            header_row = rows[0]
            header_cells = header_row.find_all(['th', 'td'])
            column_indices = {}
            
            for idx, cell in enumerate(header_cells):
                text = cell.get_text(strip=True).lower()
                if 'occurred' in text or 'date' in text:
                    column_indices['date'] = idx
                elif 'city' in text:
                    column_indices['city'] = idx
                elif 'state' in text:
                    column_indices['state'] = idx
                elif 'shape' in text:
                    column_indices['shape'] = idx
                elif 'summary' in text:
                    column_indices['summary'] = idx
                elif 'media' in text:
                    column_indices['media'] = idx
                elif 'link' in text or 'report' in text:
                    column_indices['link'] = idx
            
            # è§£ææ•°æ®è¡Œ
            page_data = []
            for row in rows[1:]:  # è·³è¿‡è¡¨å¤´
                cells = row.find_all('td')
                if len(cells) < len(header_cells):
                    continue
                
                # æå–å„å­—æ®µ
                date = cells[column_indices.get('date', 0)].get_text(strip=True) if 'date' in column_indices else ''
                city = cells[column_indices.get('city', 1)].get_text(strip=True) if 'city' in column_indices else ''
                state = cells[column_indices.get('state', 2)].get_text(strip=True) if 'state' in column_indices else ''
                shape = cells[column_indices.get('shape', 5)].get_text(strip=True) if 'shape' in column_indices else ''
                summary = cells[column_indices.get('summary', 6)].get_text(strip=True) if 'summary' in column_indices else ''
                media = cells[column_indices.get('media', 8)].get_text(strip=True) if 'media' in column_indices else ''
                
                # æå–Reporté“¾æ¥å¹¶æ£€æµ‹Tier
                report_link = ''
                is_high_tier = False
                link_cell_idx = column_indices.get('link', 0)
                if link_cell_idx < len(cells):
                    link_cell = cells[link_cell_idx]
                    
                    # æ£€æµ‹Tierï¼šæ£€æŸ¥LINKå•å…ƒæ ¼çš„èƒŒæ™¯è‰²
                    if self.is_yellow_background(link_cell):
                        is_high_tier = True
                    
                    # æå–é“¾æ¥
                    link_tag = link_cell.find('a', href=True)
                    if link_tag:
                        href = link_tag.get('href', '')
                        if '/sighting/?id=' in href:
                            report_link = urljoin(self.base_url, href)
                        
                        # æ£€æŸ¥é“¾æ¥æ–‡æœ¬ä¸­çš„ç¬¦å·ï¼ˆTieræ ‡è®°ï¼‰
                        link_text = link_tag.get_text(strip=True)
                        # æ£€æŸ¥é“¾æ¥æ–‡æœ¬ä¸­æ˜¯å¦æœ‰æ„Ÿå¹å·ï¼ˆ"Open !" è¡¨ç¤º Tier 1ï¼‰
                        if '!' in link_text:
                            is_high_tier = True
                        # æ£€æŸ¥é“¾æ¥æ–‡æœ¬ä¸­æ˜¯å¦æœ‰ç‚¹å·ï¼ˆ"Open ." è¡¨ç¤º Tier 2ï¼‰
                        elif link_text.endswith('.') or link_text == 'Open .' or 'Open .' in link_text:
                            is_high_tier = True
                        # æ£€æŸ¥é“¾æ¥å…ƒç´ æ˜¯å¦æœ‰é»„è‰²èƒŒæ™¯ï¼ˆå¤‡ç”¨æ£€æµ‹æ–¹æ³•ï¼‰
                        if self.is_yellow_background(link_tag):
                            is_high_tier = True
                
                page_data.append({
                    'Date': date,
                    'City': city,
                    'State': state,
                    'Shape': shape,
                    'Summary': summary,
                    'Media': media,
                    'Report_Link': report_link,
                    'Is_High_Tier': is_high_tier
                })
            
            return page_data
            
        except Exception as e:
            print(f"æŠ“å–åˆ†é¡µè¡¨æ ¼å¤±è´¥ (start={start}): {e}")
            return []
    
    def extract_image_from_detail_page(self, report_url):
        """ä»è¯¦æƒ…é¡µæå–å›¾ç‰‡URLå’Œç›¸å…³ä¿¡æ¯"""
        try:
            # å°è¯•å…ˆä½¿ç”¨SSLéªŒè¯
            try:
                response = self.session.get(report_url, timeout=10, verify=True)
            except Exception as e:
                # å¦‚æœSSLéªŒè¯å¤±è´¥ï¼Œå°è¯•ç¦ç”¨SSLéªŒè¯
                import urllib3
                urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
                response = self.session.get(report_url, timeout=10, verify=False)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–å›¾ç‰‡
            images = []
            img_tags = soup.find_all('img')
            
            for img in img_tags:
                src = img.get('src', '')
                if src:
                    # è·³è¿‡logoå’Œå›¾æ ‡ï¼ˆåªè¿‡æ»¤æ˜ç¡®çš„logo/icon/buttonï¼‰
                    if 'logo' in src.lower() or 'icon' in src.lower() or 'button' in src.lower():
                        continue
                    
                    # æ„å»ºå®Œæ•´URL
                    if not src.startswith('http'):
                        full_url = urljoin(self.base_url, src)
                    else:
                        full_url = src
                    
                    images.append(full_url)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œè¿”å›None
            if not images:
                return None
            
            # æå–åŸºæœ¬ä¿¡æ¯
            text_content = soup.get_text()
            date = ''
            city = ''
            state = ''
            shape = ''
            description = ''
            
            # æŸ¥æ‰¾åŒ…å«æ—¥æœŸçš„æ–‡æœ¬
            date_match = re.search(r'(\d{1,2}/\d{1,2}/\d{4})', text_content)
            if date_match:
                date = date_match.group(1)
            
            # æŸ¥æ‰¾åŸå¸‚å’Œå·
            city_match = re.search(r'City[:\s]+([^,\n]+)', text_content, re.IGNORECASE)
            if city_match:
                city = city_match.group(1).strip()
            
            state_match = re.search(r'State[:\s]+([A-Z]{2})', text_content, re.IGNORECASE)
            if state_match:
                state = state_match.group(1)
            
            # æŸ¥æ‰¾Shape
            shape_match = re.search(r'Shape[:\s]+([^\n]+)', text_content, re.IGNORECASE)
            if shape_match:
                shape = shape_match.group(1).strip()[:50]
            
            # æå–æè¿°ï¼ˆå‰500å­—ç¬¦ï¼‰
            paragraphs = soup.find_all('p')
            description_texts = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
            description = ' '.join(description_texts)[:500]
            
            return {
                'images': images,
                'date': date,
                'city': city,
                'state': state,
                'shape': shape,
                'description': description,
                'report_url': report_url
            }
            
        except Exception as e:
            return None
    
    def load_existing_images(self):
        """åŠ è½½å·²æœ‰çš„å›¾ç‰‡æ•°æ®ï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ """
        try:
            df = pd.read_csv('ufo_images.csv')
            processed_reports = set(df['Report_URL'].unique())
            existing_images = df.to_dict('records')
            print(f"ğŸ“‚ å‘ç°å·²æœ‰æ•°æ®ï¼š{len(existing_images)} å¼ å›¾ç‰‡ï¼Œ{len(processed_reports)} ä¸ªæŠ¥å‘Šå·²å¤„ç†")
            return existing_images, processed_reports
        except FileNotFoundError:
            print("ğŸ“‚ æœªæ‰¾åˆ°å·²æœ‰æ•°æ®æ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹")
            return [], set()
        except Exception as e:
            print(f"âš ï¸ è¯»å–å·²æœ‰æ•°æ®æ—¶å‡ºé”™: {e}ï¼Œå°†ä»å¤´å¼€å§‹")
            return [], set()
    
    def scrape_all_images(self):
        """ä¸»çˆ¬å–å‡½æ•°ï¼šä»æ•°æ®æ–‡ä»¶è¯»å–æ•°æ®ï¼Œç­›é€‰Media=Yçš„è®°å½•ï¼Œç„¶åè®¿é—®è¯¦æƒ…é¡µæŠ“å–å›¾ç‰‡"""
        print("=" * 60)
        print("NUFORC UFO å›¾ç‰‡å®Œæ•´çˆ¬è™«å¯åŠ¨ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰")
        print("=" * 60)
        
        # 0. åŠ è½½å·²æœ‰æ•°æ®ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        existing_images, processed_reports = self.load_existing_images()
        self.all_images = existing_images.copy() if existing_images else []
        
        # 1. ä»CSVæ–‡ä»¶è¯»å–æŠ¥å‘Šæ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨å®Œæ•´æ•°æ®æ–‡ä»¶ï¼‰
        data_file = None
        if os.path.exists('ufo_data_tiered_full.csv'):
            data_file = 'ufo_data_tiered_full.csv'
            print("\næ­£åœ¨è¯»å– ufo_data_tiered_full.csvï¼ˆå®Œæ•´æ•°æ®æ–‡ä»¶ï¼‰...")
        elif os.path.exists('ufo_data_tiered.csv'):
            data_file = 'ufo_data_tiered.csv'
            print("\næ­£åœ¨è¯»å– ufo_data_tiered.csv...")
        else:
            print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼ˆufo_data_tiered_full.csv æˆ– ufo_data_tiered.csvï¼‰")
            return
        
        try:
            all_reports_df = pd.read_csv(data_file)
            print(f"âœ… æˆåŠŸè¯»å– {len(all_reports_df)} æ¡æŠ¥å‘Šæ•°æ®")
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return
        
        # 2. ç­›é€‰æœ‰Report_Linkçš„è®°å½•ï¼Œä¸”Media = "Y"ï¼ˆåªå¤„ç†æœ‰åª’ä½“æ–‡ä»¶çš„æŠ¥å‘Šï¼‰
        reports_with_links = all_reports_df[
            all_reports_df['Report_Link'].notna() & 
            (all_reports_df['Report_Link'] != '') & 
            (all_reports_df['Media'] == 'Y')
        ].copy()
        print(f"\næ‰¾åˆ° {len(reports_with_links)} æ¡æœ‰é“¾æ¥ä¸”æœ‰åª’ä½“çš„æŠ¥å‘Šï¼ˆMedia=Yï¼‰")
        print(f"å…¶ä¸­ Tier 1/2: {reports_with_links['Is_High_Tier'].sum()} æ¡")
        print(f"æ™®é€šæŠ¥å‘Š: {(reports_with_links['Is_High_Tier'] == False).sum()} æ¡")
        
        # è¿‡æ»¤æ‰å·²å¤„ç†çš„æŠ¥å‘Š
        reports_to_process = reports_with_links[~reports_with_links['Report_Link'].isin(processed_reports)]
        skipped_count = len(reports_with_links) - len(reports_to_process)
        print(f"âœ… å·²å¤„ç†æŠ¥å‘Š: {skipped_count} ä¸ªï¼ˆå°†è·³è¿‡ï¼‰")
        print(f"ğŸ“‹ å¾…å¤„ç†æŠ¥å‘Š: {len(reports_to_process)} ä¸ª\n")
        
        if len(reports_to_process) == 0:
            print("æ‰€æœ‰æŠ¥å‘Šå·²å¤„ç†å®Œæˆï¼")
            return
        
        # 3. éå†å¾…å¤„ç†çš„æŠ¥å‘Šï¼Œæå–å›¾ç‰‡ï¼ˆæ— æ—¶é—´é™åˆ¶ï¼Œå®Œæ•´æŠ“å–ï¼‰
        print("å¼€å§‹æŠ“å–å›¾ç‰‡...")
        print("âš ï¸ å®Œæ•´æ¨¡å¼ï¼šå°†éå†æ‰€æœ‰å¾…å¤„ç†æŠ¥å‘Šï¼ˆå¯èƒ½éœ€è¦æ•°å°æ—¶ï¼‰")
        print("å»ºè®®ï¼šå¦‚æœéœ€è¦ä¸­æ–­ï¼ŒæŒ‰ Ctrl+C ä¿å­˜å·²æŠ“å–çš„æ•°æ®\n")
        
        processed_count = 0
        error_count = 0
        
        for idx, row in tqdm(reports_to_process.iterrows(), total=len(reports_to_process), desc="æŠ“å–æŠ¥å‘Š", unit="ä¸ª"):
            report_url = row['Report_Link']
            is_high_tier = bool(row.get('Is_High_Tier', False))
            
            try:
                # è®¿é—®è¯¦æƒ…é¡µæå–å›¾ç‰‡
                image_data = self.extract_image_from_detail_page(report_url)
                
                if image_data and image_data['images']:
                    # ä¸ºæ¯å¼ å›¾ç‰‡åˆ›å»ºä¸€æ¡è®°å½•
                    for img_url in image_data['images']:
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤ï¼‰
                        if not any(existing['Image_URL'] == img_url for existing in self.all_images):
                            self.all_images.append({
                                'Image_URL': img_url,
                                'Report_URL': report_url,
                                'Date': image_data.get('date', row.get('Date', '')),
                                'City': image_data.get('city', row.get('City', '')),
                                'State': image_data.get('state', row.get('State', '')),
                                'Shape': image_data.get('shape', row.get('Shape', '')),
                                'Summary': row.get('Summary', ''),
                                'Description': image_data.get('description', ''),
                                'Is_High_Tier': is_high_tier,
                                'Tier': 'Tier 1/2' if is_high_tier else 'Normal'
                            })
                    processed_count += 1
                else:
                    error_count += 1
                    
            except KeyboardInterrupt:
                print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å·²æŠ“å–çš„æ•°æ®...")
                break
            except Exception as e:
                error_count += 1
                continue
            
            # æ¯æ¬¡è¯·æ±‚åä¼‘æ¯0.5ç§’ï¼ˆé˜²æ­¢è¢«å°IPï¼‰
            time.sleep(0.5)
        
        # 4. ä¿å­˜æ•°æ®ï¼ˆåˆå¹¶æ–°æ—§æ•°æ®ï¼‰
        if not self.all_images:
            print("\næœªè·å–åˆ°ä»»ä½•å›¾ç‰‡æ•°æ®")
            return
        
        print("\næ­£åœ¨ä¿å­˜æ•°æ®...")
        df = pd.DataFrame(self.all_images)
        
        # å»é‡ï¼ˆå¦‚æœæœ‰é‡å¤çš„å›¾ç‰‡URLï¼‰
        df = df.drop_duplicates(subset=['Image_URL'], keep='first')
        
        # ç¡®ä¿åˆ—çš„é¡ºåº
        columns_order = ['Image_URL', 'Report_URL', 'Date', 'City', 'State', 'Shape', 'Summary', 'Description', 'Is_High_Tier', 'Tier']
        df = df[columns_order]
        
        # ä¿å­˜æ•°æ®
        output_file = 'ufo_images.csv'
        df.to_csv(output_file, index=False, encoding='utf-8')
        
        # è¾“å‡ºç»Ÿè®¡
        new_images_count = len(self.all_images) - len(existing_images) if existing_images else len(self.all_images)
        print("\n" + "=" * 60)
        print("âœ… æŠ“å–å®Œæˆï¼")
        print(f"ğŸ“Š æœ¬æ¬¡æ–°å¢å›¾ç‰‡: {new_images_count} å¼ ")
        print(f"ğŸ“Š ç´¯è®¡æ€»å›¾ç‰‡æ•°: {len(df)} å¼ ")
        print(f"â­ Tier 1/2 å›¾ç‰‡: {df['Is_High_Tier'].sum()} å¼  ({df['Is_High_Tier'].sum()/len(df)*100:.2f}%)")
        print(f"ğŸ“¸ æ™®é€šå›¾ç‰‡: {(df['Is_High_Tier'] == False).sum()} å¼  ({(df['Is_High_Tier'] == False).sum()/len(df)*100:.2f}%)")
        print(f"âœ… æœ¬æ¬¡æˆåŠŸå¤„ç†æŠ¥å‘Š: {processed_count} ä¸ª")
        print(f"âŒ æœ¬æ¬¡å¤±è´¥/æ— å›¾ç‰‡æŠ¥å‘Š: {error_count} ä¸ª")
        print(f"ğŸ“‹ å‰©ä½™å¾…å¤„ç†æŠ¥å‘Š: {len(reports_with_links) - len(processed_reports) - processed_count} ä¸ª")
        print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜è‡³ {output_file}")
        print("=" * 60)


def main():
    scraper = UFOImageScraper()
    scraper.scrape_all_images()


if __name__ == "__main__":
    main()
