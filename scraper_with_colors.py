"""
NUFORC UFO æŠ¥å‘Šçˆ¬è™«ï¼ˆå¸¦Tieræ£€æµ‹ç‰ˆæœ¬ï¼‰
æ£€æµ‹è¡¨æ ¼ä¸­é»„è‰²èƒŒæ™¯çš„å•å…ƒæ ¼æ¥åˆ¤æ–­Tier 1/2æ¡ˆä»¶
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from urllib.parse import urljoin
from tqdm import tqdm


class UFOTierScraper:
    def __init__(self):
        self.base_url = "https://nuforc.org"
        self.index_url = "https://nuforc.org/webreports/ndxevent.html"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.all_data = []
        
    def get_month_links(self):
        """
        è·å–æ‰€æœ‰æœˆä»½é“¾æ¥ï¼ŒæŒ‰æ—¶é—´å€’åºæ’åˆ—
        """
        try:
            print("æ­£åœ¨è·å–æœˆä»½ç´¢å¼•...")
            response = self.session.get(self.index_url, timeout=10)
            response.raise_for_status()
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æœˆä»½é“¾æ¥
            pattern = r'href=["\']?([^"\'>\s]*/subndx/\?id=e\d+)["\']?'
            matches = re.findall(pattern, response.text, re.IGNORECASE)
            
            month_links = []
            for href in matches:
                if not href.startswith('http'):
                    full_url = urljoin(self.base_url, href)
                else:
                    full_url = href
                month_links.append(full_url)
            
            # å»é‡å¹¶æ’åºï¼ˆå€’åºï¼Œæœ€æ–°çš„åœ¨å‰ï¼‰
            month_links = list(set(month_links))
            month_links.sort(reverse=True)
            
            print(f"æ‰¾åˆ° {len(month_links)} ä¸ªæœˆä»½é“¾æ¥")
            return month_links
            
        except Exception as e:
            print(f"è·å–æœˆä»½é“¾æ¥å¤±è´¥: {e}")
            return []
    
    def is_yellow_background(self, element):
        """
        æ£€æµ‹å…ƒç´ æ˜¯å¦æœ‰é»„è‰²æˆ–æ·¡é»„è‰²èƒŒæ™¯
        æ£€æŸ¥styleå±æ€§å’Œbgcolorå±æ€§
        """
        if not element:
            return False
        
        # è·å–styleå±æ€§
        style = element.get('style', '')
        bgcolor = element.get('bgcolor', '')
        
        # åˆå¹¶æ‰€æœ‰é¢œè‰²ä¿¡æ¯
        all_color_info = (style + ' ' + bgcolor).lower()
        
        # æ£€æŸ¥é»„è‰²å…³é”®è¯ï¼ˆåŒ…æ‹¬å„ç§å¯èƒ½çš„é»„è‰²å˜ä½“ï¼‰
        yellow_keywords = [
            'yellow',
            '#ffff00',  # çº¯é»„è‰²
            '#ffffc0',  # æ·¡é»„è‰²
            '#ffffcc',  # æ·¡é»„è‰²å˜ä½“
            '#ffff99',  # æ·¡é»„è‰²å˜ä½“
            '#ffffe0',  # æ·¡é»„è‰²å˜ä½“
            '#ffffd0',  # æ·¡é»„è‰²å˜ä½“
            '#ffffb0',  # æ·¡é»„è‰²å˜ä½“
            '#fffacd',  # æŸ æª¬é›ªçººè‰²
            '#fff8dc',  # ç‰ç±³è‰²
            '#ffeb3b',  # äº®é»„è‰²
            '#ffc107',  # ç¥ç€è‰²ï¼ˆåé»„ï¼‰
            'rgb(255, 255, 0)',  # RGBé»„è‰²
            'rgb(255, 255, 192)',  # RGBæ·¡é»„è‰²
            'rgb(255, 255, 204)',  # RGBæ·¡é»„è‰²å˜ä½“
            'rgb(255, 255, 176)',  # RGBæ·¡é»„è‰²å˜ä½“
            'rgb(255, 255, 224)',  # RGBæ·¡é»„è‰²å˜ä½“
        ]
        
        for keyword in yellow_keywords:
            if keyword in all_color_info:
                return True
        
        # æ£€æŸ¥RGBæ ¼å¼ (rgb(255, 255, x) å…¶ä¸­x < 200è¡¨ç¤ºé»„è‰²)
        rgb_match = re.search(r'rgb\((\d+),\s*(\d+),\s*(\d+)\)', all_color_info)
        if rgb_match:
            r, g, b = int(rgb_match.group(1)), int(rgb_match.group(2)), int(rgb_match.group(3))
            # é»„è‰²ç‰¹å¾ï¼šRå’ŒGéƒ½å¾ˆé«˜(>200)ï¼ŒBè¾ƒä½(<200)
            if r > 200 and g > 200 and b < 200:
                return True
        
        return False
    
    def scrape_month_table(self, month_url):
        """
        ä½¿ç”¨BeautifulSoupè§£æè¡¨æ ¼ï¼Œæ£€æµ‹é»„è‰²èƒŒæ™¯
        """
        try:
            response = self.session.get(month_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            table = soup.find('table')
            
            if not table:
                return []
            
            rows = table.find_all('tr')
            if len(rows) < 2:  # è‡³å°‘éœ€è¦è¡¨å¤´+1è¡Œæ•°æ®
                return []
            
            # è§£æè¡¨å¤´ï¼Œæ‰¾åˆ°å„åˆ—çš„ç´¢å¼•
            header_row = rows[0]
            header_cells = header_row.find_all(['th', 'td'])
            column_indices = {}
            
            for idx, cell in enumerate(header_cells):
                text = cell.get_text(strip=True).lower()
                if 'occurred' in text or 'date' in text:
                    column_indices['date'] = idx
                elif 'city' in text:
                    column_indices['city'] = idx
                elif 'state' in text:
                    column_indices['state'] = idx
                elif 'shape' in text:
                    column_indices['shape'] = idx
                elif 'summary' in text:
                    column_indices['summary'] = idx
                elif 'media' in text:
                    column_indices['media'] = idx
                elif 'link' in text:
                    column_indices['link'] = idx
            
            # è§£ææ•°æ®è¡Œ
            month_data = []
            for row in rows[1:]:  # è·³è¿‡è¡¨å¤´
                cells = row.find_all('td')
                if len(cells) < len(header_cells):
                    continue
                
                # æå–å„å­—æ®µ
                date = cells[column_indices.get('date', 0)].get_text(strip=True) if 'date' in column_indices else ''
                city = cells[column_indices.get('city', 1)].get_text(strip=True) if 'city' in column_indices else ''
                state = cells[column_indices.get('state', 2)].get_text(strip=True) if 'state' in column_indices else ''
                shape = cells[column_indices.get('shape', 5)].get_text(strip=True) if 'shape' in column_indices else ''
                summary = cells[column_indices.get('summary', 6)].get_text(strip=True) if 'summary' in column_indices else ''
                media = cells[column_indices.get('media', 8)].get_text(strip=True) if 'media' in column_indices else ''
                
                # æå–Reporté“¾æ¥ï¼ˆé€šå¸¸åœ¨ç¬¬ä¸€ä¸ªå•å…ƒæ ¼ï¼‰
                report_link = ''
                link_cell_idx = column_indices.get('link', 0)
                if link_cell_idx < len(cells):
                    link_cell = cells[link_cell_idx]
                    link_tag = link_cell.find('a', href=True)
                    if link_tag:
                        href = link_tag.get('href', '')
                        if '/sighting/?id=' in href:
                            report_link = urljoin(self.base_url, href)
                
                # æ£€æµ‹Tierï¼šæ£€æŸ¥é“¾æ¥æ–‡æœ¬ä¸­çš„ç¬¦å·
                # Tier 1: "Open !" (æœ‰æ„Ÿå¹å·)
                # Tier 2: "Open ." (æœ‰ç‚¹å·)
                is_high_tier = False
                if link_cell_idx < len(cells):
                    link_cell = cells[link_cell_idx]
                    # æ£€æŸ¥å•å…ƒæ ¼æœ¬èº«æ˜¯å¦æœ‰é»„è‰²èƒŒæ™¯
                    if self.is_yellow_background(link_cell):
                        is_high_tier = True
                    # æ£€æŸ¥å•å…ƒæ ¼å†…çš„é“¾æ¥
                    link_tag = link_cell.find('a')
                    if link_tag:
                        link_text = link_tag.get_text(strip=True)
                        # æ£€æŸ¥é“¾æ¥æ–‡æœ¬ä¸­æ˜¯å¦æœ‰æ„Ÿå¹å·ï¼ˆ"Open !" è¡¨ç¤º Tier 1ï¼‰
                        if '!' in link_text:
                            is_high_tier = True
                        # æ£€æŸ¥é“¾æ¥æ–‡æœ¬ä¸­æ˜¯å¦æœ‰ç‚¹å·ï¼ˆ"Open ." è¡¨ç¤º Tier 2ï¼‰
                        # æ³¨æ„ï¼šç‚¹å·å¯èƒ½åœ¨æ–‡æœ¬æœ«å°¾ï¼Œæ ¼å¼ä¸º "Open ."
                        elif link_text.endswith('.') or link_text == 'Open .' or 'Open .' in link_text:
                            is_high_tier = True
                        # æ£€æŸ¥é“¾æ¥å…ƒç´ æ˜¯å¦æœ‰é»„è‰²èƒŒæ™¯ï¼ˆå¤‡ç”¨æ£€æµ‹æ–¹æ³•ï¼‰
                        if self.is_yellow_background(link_tag):
                            is_high_tier = True
                
                month_data.append({
                    'Date': date,
                    'City': city,
                    'State': state,
                    'Shape': shape,
                    'Summary': summary,
                    'Media': media,
                    'Report_Link': report_link,
                    'Is_High_Tier': is_high_tier
                })
            
            return month_data
            
        except Exception as e:
            print(f"\næŠ“å–å¤±è´¥ ({month_url}): {e}")
            return []
    
    def scrape_all(self):
        """
        ä¸»çˆ¬å–å‡½æ•°
        """
        print("=" * 60)
        print("NUFORC UFO æŠ¥å‘Šçˆ¬è™«ï¼ˆå¸¦Tieræ£€æµ‹ï¼‰å¯åŠ¨")
        print("=" * 60)
        
        # 1. è·å–æ‰€æœ‰æœˆä»½é“¾æ¥
        month_links = self.get_month_links()
        if not month_links:
            print("æœªæ‰¾åˆ°æœˆä»½é“¾æ¥ï¼Œç¨‹åºé€€å‡º")
            return
        
        # 2. éå†æ‰€æœ‰æœˆä»½ï¼Œè§£æè¡¨æ ¼å¹¶æ£€æµ‹Tier
        print("\nå¼€å§‹æŠ“å–æ•°æ®...")
        for month_url in tqdm(month_links, desc="æŠ“å–æœˆä»½", unit="ä¸ªæœˆ"):
            month_data = self.scrape_month_table(month_url)
            self.all_data.extend(month_data)
            
            # æ¯æ¬¡è¯·æ±‚åä¼‘æ¯0.5ç§’
            time.sleep(0.5)
        
        # 3. è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
        if not self.all_data:
            print("\næœªè·å–åˆ°ä»»ä½•æ•°æ®")
            return
        
        print("\næ­£åœ¨ä¿å­˜æ•°æ®...")
        df = pd.DataFrame(self.all_data)
        
        # ç¡®ä¿åˆ—çš„é¡ºåº
        columns_order = ['Date', 'City', 'State', 'Shape', 'Summary', 'Media', 'Report_Link', 'Is_High_Tier']
        df = df[columns_order]
        
        # ä¿å­˜æ•°æ®
        output_file = 'ufo_data_tiered.csv'
        df.to_csv(output_file, index=False, encoding='utf-8')
        
        # è¾“å‡ºç»Ÿè®¡
        print("\n" + "=" * 60)
        print("âœ… æŠ“å–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»å…±è·å–äº† {len(df)} æ¡æ•°æ®")
        print(f"â­ Tier 1/2 æ¡ˆä»¶: {df['Is_High_Tier'].sum()} æ¡ ({df['Is_High_Tier'].sum()/len(df)*100:.2f}%)")
        print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜è‡³ {output_file}")
        print("=" * 60)


def main():
    scraper = UFOTierScraper()
    scraper.scrape_all()


if __name__ == "__main__":
    main()

