"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯Report_Linkæå–é€»è¾‘
"""
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time

try:
    from webdriver_manager.chrome import ChromeDriverManager
    WEBDRIVER_MANAGER_AVAILABLE = True
except ImportError:
    WEBDRIVER_MANAGER_AVAILABLE = False

def is_yellow_background(element):
    """æ£€æµ‹å…ƒç´ æ˜¯å¦æœ‰é»„è‰²èƒŒæ™¯"""
    if not element:
        return False
    
    style = element.get('style', '')
    bgcolor = element.get('bgcolor', '')
    all_color_info = (style + ' ' + bgcolor).lower()
    
    yellow_keywords = ['yellow', '#ffff00', '#ffffc0', '#ffffcc', '#ffff99']
    for keyword in yellow_keywords:
        if keyword in all_color_info:
            return True
    return False

def test_link_extraction():
    """æµ‹è¯•é“¾æ¥æå–é€»è¾‘"""
    print('=' * 60)
    print('æµ‹è¯•Report_Linkæå–é€»è¾‘')
    print('=' * 60)
    
    # åˆå§‹åŒ–æµè§ˆå™¨
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    
    try:
        if WEBDRIVER_MANAGER_AVAILABLE:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
        else:
            driver = webdriver.Chrome(options=chrome_options)
        
        print('\n1. æ­£åœ¨è®¿é—®é¡µé¢...')
        driver.get('https://nuforc.org/subndx/?id=all')
        time.sleep(10)
        
        print('\n2. è§£æç¬¬ä¸€é¡µæ•°æ®...')
        wait = WebDriverWait(driver, 30)
        wait.until(EC.presence_of_element_located((By.TAG_NAME, "table")))
        
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        table = soup.find('table')
        
        if not table:
            print('âŒ æœªæ‰¾åˆ°è¡¨æ ¼')
            return
        
        rows = table.find_all('tr')
        if len(rows) < 2:
            print('âŒ è¡¨æ ¼æ•°æ®ä¸è¶³')
            return
        
        # è§£æå‰5è¡Œæ•°æ®
        print('\n3. æµ‹è¯•é“¾æ¥æå–ï¼ˆå‰5è¡Œæ•°æ®ï¼‰:')
        success_count = 0
        total_count = min(5, len(rows) - 1)
        
        for i in range(1, total_count + 1):
            row = rows[i]
            cells = row.find_all('td')
            
            # ä½¿ç”¨ä¿®å¤åçš„é€»è¾‘ï¼šéå†æ‰€æœ‰å•å…ƒæ ¼æŸ¥æ‰¾é“¾æ¥
            report_link = ''
            for cell in cells:
                link_tag = cell.find('a', href=True)
                if link_tag:
                    href = link_tag.get('href', '')
                    if '/sighting/?id=' in href:
                        report_link = urljoin('https://nuforc.org', href)
                        break
            
            if report_link:
                success_count += 1
                print(f'   è¡Œ{i}: âœ… æˆåŠŸæå–é“¾æ¥: {report_link[:60]}...')
            else:
                print(f'   è¡Œ{i}: âŒ æœªæ‰¾åˆ°é“¾æ¥')
                # è°ƒè¯•ï¼šæ˜¾ç¤ºæ‰€æœ‰å•å…ƒæ ¼çš„å†…å®¹
                for idx, cell in enumerate(cells):
                    text = cell.get_text(strip=True)[:30]
                    links = cell.find_all('a', href=True)
                    print(f'      åˆ—{idx}: "{text}" (é“¾æ¥æ•°: {len(links)})')
        
        print(f'\nâœ… æµ‹è¯•å®Œæˆ: {success_count}/{total_count} è¡ŒæˆåŠŸæå–é“¾æ¥')
        
        if success_count == total_count:
            print('ğŸ‰ é“¾æ¥æå–é€»è¾‘æ­£å¸¸ï¼å¯ä»¥è¿è¡Œå®Œæ•´çˆ¬å–')
        else:
            print('âš ï¸ éƒ¨åˆ†è¡Œæœªèƒ½æå–é“¾æ¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•')
        
        driver.quit()
        
    except Exception as e:
        print(f'\nâŒ æµ‹è¯•å¤±è´¥: {e}')
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_link_extraction()
