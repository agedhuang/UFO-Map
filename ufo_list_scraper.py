"""
NUFORC UFO æŠ¥å‘Šåˆ—è¡¨çˆ¬è™«ï¼ˆæé€Ÿç‰ˆï¼‰
ä½¿ç”¨ pd.read_html() å¿«é€Ÿè·å–æ‰€æœ‰å†å²æ•°æ®æ¦‚è§ˆ
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from urllib.parse import urljoin
from tqdm import tqdm
from io import StringIO


class UFOListScraper:
    def __init__(self):
        self.base_url = "https://nuforc.org"
        self.index_url = "https://nuforc.org/webreports/ndxevent.html"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.all_dataframes = []
        
    def get_month_links(self):
        """
        è·å–æ‰€æœ‰æœˆä»½é“¾æ¥ï¼ŒæŒ‰æ—¶é—´å€’åºæ’åˆ—
        """
        try:
            print("æ­£åœ¨è·å–æœˆä»½ç´¢å¼•...")
            response = self.session.get(self.index_url, timeout=10)
            response.raise_for_status()
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æœˆä»½é“¾æ¥
            pattern = r'href=["\']?([^"\'>\s]*/subndx/\?id=e\d+)["\']?'
            matches = re.findall(pattern, response.text, re.IGNORECASE)
            
            month_links = []
            for href in matches:
                if not href.startswith('http'):
                    full_url = urljoin(self.base_url, href)
                else:
                    full_url = href
                month_links.append(full_url)
            
            # å»é‡å¹¶æ’åºï¼ˆå€’åºï¼Œæœ€æ–°çš„åœ¨å‰ï¼‰
            month_links = list(set(month_links))
            month_links.sort(reverse=True)
            
            print(f"æ‰¾åˆ° {len(month_links)} ä¸ªæœˆä»½é“¾æ¥")
            return month_links
            
        except Exception as e:
            print(f"è·å–æœˆä»½é“¾æ¥å¤±è´¥: {e}")
            return []
    
    def scrape_month_table(self, month_url):
        """
        ä½¿ç”¨ pd.read_html() å¿«é€Ÿè¯»å–æœˆä»½é¡µé¢çš„è¡¨æ ¼
        """
        try:
            response = self.session.get(month_url, timeout=10)
            response.raise_for_status()
            
            # æå–è¡¨æ ¼HTMLå¹¶æ¸…ç†
            soup = BeautifulSoup(response.text, 'html.parser')
            table = soup.find('table')
            
            if not table:
                return None
            
            # ç§»é™¤styleå±æ€§ä¸­çš„display:noneï¼Œè®©pd.read_htmlèƒ½è¯†åˆ«
            if 'style' in table.attrs:
                style = table.attrs['style']
                style = style.replace('display: none;', '').replace('display:none;', '')
                if style.strip():
                    table.attrs['style'] = style
                else:
                    del table.attrs['style']
            
            # æå–é“¾æ¥ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦Report_Linkåˆ—ï¼‰
            links_list = []
            rows = table.find_all('tr')
            for row in rows[1:]:  # è·³è¿‡è¡¨å¤´
                link_tag = row.find('a', href=True)
                if link_tag:
                    href = link_tag.get('href', '')
                    if '/sighting/?id=' in href:
                        full_link = urljoin(self.base_url, href)
                        links_list.append(full_link)
                    else:
                        links_list.append('')
                else:
                    links_list.append('')
            
            # ä½¿ç”¨StringIOåŒ…è£…HTMLï¼Œè®©pd.read_htmlè¯»å–
            table_html = str(table)
            tables = pd.read_html(StringIO(table_html))
            
            if not tables:
                return None
            
            df = tables[0]
            
            # æ·»åŠ Report_Linkåˆ—
            if links_list and len(links_list) == len(df):
                df['Report_Link'] = links_list
            else:
                df['Report_Link'] = ''
            
            return df
            
        except Exception as e:
            print(f"\næŠ“å–å¤±è´¥ ({month_url}): {e}")
            return None
    
    def scrape_all(self):
        """
        ä¸»çˆ¬å–å‡½æ•°
        """
        print("=" * 60)
        print("NUFORC UFO æŠ¥å‘Šåˆ—è¡¨çˆ¬è™«ï¼ˆæé€Ÿç‰ˆï¼‰å¯åŠ¨")
        print("=" * 60)
        
        # 1. è·å–æ‰€æœ‰æœˆä»½é“¾æ¥
        month_links = self.get_month_links()
        if not month_links:
            print("æœªæ‰¾åˆ°æœˆä»½é“¾æ¥ï¼Œç¨‹åºé€€å‡º")
            return
        
        # 2. éå†æ‰€æœ‰æœˆä»½ï¼Œä½¿ç”¨pd.read_htmlå¿«é€Ÿè¯»å–è¡¨æ ¼
        print("\nå¼€å§‹æŠ“å–æ•°æ®...")
        for month_url in tqdm(month_links, desc="æŠ“å–æœˆä»½", unit="ä¸ªæœˆ"):
            df = self.scrape_month_table(month_url)
            if df is not None and not df.empty:
                self.all_dataframes.append(df)
            
            # æ¯æ¬¡è¯·æ±‚åä¼‘æ¯0.5ç§’
            time.sleep(0.5)
        
        # 3. åˆå¹¶æ‰€æœ‰æ•°æ®
        if not self.all_dataframes:
            print("\næœªè·å–åˆ°ä»»ä½•æ•°æ®")
            return
        
        print("\næ­£åœ¨åˆå¹¶æ•°æ®...")
        combined_df = pd.concat(self.all_dataframes, ignore_index=True)
        
        # 4. æ•°æ®æ¸…æ´—ï¼šç»Ÿä¸€åˆ—å
        combined_df = self.clean_columns(combined_df)
        
        # 5. ä¿å­˜æ•°æ®
        output_file = 'ufo_list_full.csv'
        combined_df.to_csv(output_file, index=False, encoding='utf-8')
        
        # 6. æ‰“å°ç»“æœ
        print("\n" + "=" * 60)
        print("âœ… æŠ“å–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»å…±è·å–äº† {len(combined_df)} æ¡æ•°æ®")
        print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜è‡³ {output_file}")
        print("=" * 60)
    
    def clean_columns(self, df):
        """
        æ•°æ®æ¸…æ´—ï¼šç»Ÿä¸€åˆ—åä¸ºæŒ‡å®šæ ¼å¼
        ç›®æ ‡åˆ—åï¼šDate, City, State, Shape, Duration, Summary, Posted
        """
        # åˆ›å»ºåˆ—åæ˜ å°„å­—å…¸ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        column_mapping = {}
        
        for col in df.columns:
            col_lower = str(col).strip().lower()
            
            # æ˜ å°„åˆ°ç›®æ ‡åˆ—å
            if 'occurred' in col_lower or ('date' in col_lower and 'reported' not in col_lower):
                column_mapping[col] = 'Date'
            elif 'city' in col_lower:
                column_mapping[col] = 'City'
            elif 'state' in col_lower:
                column_mapping[col] = 'State'
            elif 'shape' in col_lower:
                column_mapping[col] = 'Shape'
            elif 'duration' in col_lower:
                column_mapping[col] = 'Duration'
            elif 'summary' in col_lower:
                column_mapping[col] = 'Summary'
            elif 'posted' in col_lower:
                column_mapping[col] = 'Posted'
            elif 'reported' in col_lower and 'posted' not in col_lower:
                # å¦‚æœæ²¡æœ‰Postedåˆ—ï¼Œä½¿ç”¨Reportedåˆ—
                column_mapping[col] = 'Posted'
        
        # é‡å‘½ååˆ—
        df = df.rename(columns=column_mapping)
        
        # ç¡®ä¿ç›®æ ‡åˆ—å­˜åœ¨ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºç©ºåˆ—ï¼‰
        target_columns = ['Date', 'City', 'State', 'Shape', 'Duration', 'Summary', 'Posted']
        for col in target_columns:
            if col not in df.columns:
                df[col] = ''
        
        # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåºï¼šç›®æ ‡åˆ—åœ¨å‰ï¼Œç„¶åæ˜¯Report_Linkï¼Œæœ€åæ˜¯å…¶ä»–åˆ—
        other_columns = [col for col in df.columns if col not in target_columns and col != 'Report_Link']
        final_columns = target_columns + ['Report_Link'] + other_columns
        final_columns = [col for col in final_columns if col in df.columns]
        df = df[final_columns]
        
        return df


def main():
    scraper = UFOListScraper()
    scraper.scrape_all()


if __name__ == "__main__":
    main()

